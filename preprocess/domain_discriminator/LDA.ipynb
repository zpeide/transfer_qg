{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import json\n",
    "import smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 89453, 18613, 11679\n"
     ]
    }
   ],
   "source": [
    "def extract_documents(ds_path):\n",
    "    ds = [' '.join(json.loads(_)['text']) for _ in open(ds_path)]\n",
    "    return ds\n",
    "\n",
    "nq_docs = list(extract_documents(\"../../data/nq/train.jsonl\"))\n",
    "race_docs = list(extract_documents(\"../../data/RACE/train.jsonl\"))\n",
    "sciq_docs = list(extract_documents(\"../../data/SciQ/train.jsonl\"))\n",
    "print(f\"number of docs: {len(nq_docs)}, {len(race_docs)}, {len(sciq_docs)}\")\n",
    "docs = nq_docs + race_docs + sciq_docs\n",
    "\n",
    "nq_test_docs = list(extract_documents(\"../../data/nq/dev.jsonl\"))\n",
    "race_test_docs = list(extract_documents(\"../../data/RACE/dev.jsonl\"))\n",
    "sciq_test_docs = list(extract_documents(\"../../data/SciQ/test.jsonl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_ds = open(\"../../data/nq/train.jsonl\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "def split_tokens(docs):\n",
    "    # Split the documents into tokens.\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    docs = [[token for token in doc if not token.isnumeric() and token not in stop_words] for doc in docs]\n",
    "\n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    return docs\n",
    "nq_docs = split_tokens(nq_docs)\n",
    "race_docs = split_tokens(race_docs)\n",
    "sciq_docs = split_tokens(sciq_docs)\n",
    "docs = split_tokens(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_test_docs = split_tokens(nq_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_test_docs = split_tokens(race_test_docs)\n",
    "sciq_test_docs = split_tokens(sciq_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "nq_docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in nq_docs]\n",
    "race_docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in race_docs]\n",
    "sciq_docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in sciq_docs]\n",
    "race_test_docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in race_test_docs]\n",
    "sciq_test_docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in sciq_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_test_docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in nq_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssd/peide/anaconda3/envs/cogqg/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "def add_bigrams(docs):\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "    bigram = Phrases(docs, min_count=20)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "add_bigrams(docs)\n",
    "add_bigrams(nq_docs)\n",
    "add_bigrams(race_docs)\n",
    "add_bigrams(sciq_docs)\n",
    "add_bigrams(race_test_docs)\n",
    "add_bigrams(sciq_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bigrams(nq_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "def get_dictionary(docs):\n",
    "    # Create a dictionary representation of the documents.\n",
    "    dictionary = Dictionary(docs)\n",
    "\n",
    "    # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "    return dictionary\n",
    "dictionary = get_dictionary(docs+nq_docs+race_test_docs+sciq_test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents. need to use the same dictionary.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "nq_corpus = [dictionary.doc2bow(doc) for doc in nq_docs]\n",
    "race_corpus = [dictionary.doc2bow(doc) for doc in race_docs]\n",
    "sciq_corpus = [dictionary.doc2bow(doc) for doc in sciq_docs]\n",
    "race_test_corpus = [dictionary.doc2bow(doc) for doc in race_test_docs]\n",
    "sciq_test_corpus = [dictionary.doc2bow(doc) for doc in sciq_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_test_corpus = [dictionary.doc2bow(doc) for doc in nq_test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 34838\n",
      "Number of documents: 89453, 18613, 11679, 1036, 1000\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print(f'Number of documents: {len(nq_corpus)}, {len(race_corpus)}, {len(sciq_corpus)}, {len(race_test_corpus)}, {len(sciq_test_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -3.6039.\n",
      "[([(0.012310617, 'people'),\n",
      "   (0.011563973, 'said'),\n",
      "   (0.01077379, 'like'),\n",
      "   (0.010208301, 'one'),\n",
      "   (0.008664568, 'time'),\n",
      "   (0.008580215, 'school'),\n",
      "   (0.008215999, 'day'),\n",
      "   (0.007836241, 'get'),\n",
      "   (0.007749909, 'child'),\n",
      "   (0.0073356824, 'make'),\n",
      "   (0.007298846, 'go'),\n",
      "   (0.0071585346, 'life'),\n",
      "   (0.0067995302, 'say'),\n",
      "   (0.006612989, 'would'),\n",
      "   (0.006522541, 'good'),\n",
      "   (0.0060412483, 'thing'),\n",
      "   (0.0058971485, 'see'),\n",
      "   (0.005868483, 'take'),\n",
      "   (0.0058029527, 'help'),\n",
      "   (0.0057608425, 'way')],\n",
      "  -1.7604466637069196),\n",
      " ([(0.019915888, 'cell'),\n",
      "   (0.013187877, 'energy'),\n",
      "   (0.010464404, 'water'),\n",
      "   (0.010382532, 'figure'),\n",
      "   (0.009793314, 'body'),\n",
      "   (0.009465443, 'food'),\n",
      "   (0.009312558, 'called'),\n",
      "   (0.008784117, 'also'),\n",
      "   (0.008638417, 'may'),\n",
      "   (0.008340241, 'example'),\n",
      "   (0.008281657, 'plant'),\n",
      "   (0.008275462, 'form'),\n",
      "   (0.0073485067, 'blood'),\n",
      "   (0.0065326546, 'molecule'),\n",
      "   (0.0062180455, 'system'),\n",
      "   (0.006217712, 'type'),\n",
      "   (0.006153884, 'animal'),\n",
      "   (0.0059699444, 'different'),\n",
      "   (0.0056187916, 'human'),\n",
      "   (0.005569475, 'change')],\n",
      "  -2.464182711267675),\n",
      " ([(0.023958948, 'new'),\n",
      "   (0.019930232, 'year'),\n",
      "   (0.011902791, 'first'),\n",
      "   (0.011290024, 'city'),\n",
      "   (0.0107627865, 'day'),\n",
      "   (0.010679112, 'name'),\n",
      "   (0.010403944, 'book'),\n",
      "   (0.008320193, 'many'),\n",
      "   (0.008037612, 'one'),\n",
      "   (0.007993463, 'world'),\n",
      "   (0.006678805, 'time'),\n",
      "   (0.0058777416, 'white'),\n",
      "   (0.005867191, 'early'),\n",
      "   (0.005818682, 'also'),\n",
      "   (0.0052976967, 'building'),\n",
      "   (0.0047558234, 'great'),\n",
      "   (0.0047443626, 'saw'),\n",
      "   (0.004716506, 'place'),\n",
      "   (0.0046822852, 'red'),\n",
      "   (0.00459074, 'visit')],\n",
      "  -2.6089345951560694),\n",
      " ([(0.008920261, 'use'),\n",
      "   (0.0083827255, 'student'),\n",
      "   (0.0066469316, 'u'),\n",
      "   (0.0064243446, 'time'),\n",
      "   (0.006143648, 'university'),\n",
      "   (0.006101378, 'computer'),\n",
      "   (0.0058618155, 'first'),\n",
      "   (0.005524061, 'company'),\n",
      "   (0.005482554, 'work'),\n",
      "   (0.005478354, 'one'),\n",
      "   (0.00528366, 'information'),\n",
      "   (0.005163717, 'brain'),\n",
      "   (0.005069003, 'environment'),\n",
      "   (0.0043785674, 'also'),\n",
      "   (0.0042077326, 'car'),\n",
      "   (0.0041464698, 'may'),\n",
      "   (0.004129595, 'science'),\n",
      "   (0.0040788064, 'solution'),\n",
      "   (0.0040024216, 'looked'),\n",
      "   (0.003995072, 'system')],\n",
      "  -2.8301776248060406),\n",
      " ([(0.03193983, 'state'),\n",
      "   (0.0175697, 'united'),\n",
      "   (0.013602061, 'united_state'),\n",
      "   (0.009724466, 'government'),\n",
      "   (0.009504653, 'law'),\n",
      "   (0.008861662, 'war'),\n",
      "   (0.008459211, 'american'),\n",
      "   (0.0076912134, 'remember'),\n",
      "   (0.007290295, 'sure'),\n",
      "   (0.0070147915, 'president'),\n",
      "   (0.0067245294, 'british'),\n",
      "   (0.006365143, 'interesting'),\n",
      "   (0.006046853, 'force'),\n",
      "   (0.0056041405, 'act'),\n",
      "   (0.005561453, 'first'),\n",
      "   (0.005466113, 'national'),\n",
      "   (0.005061924, 'photo'),\n",
      "   (0.0047342824, 'right'),\n",
      "   (0.004678062, 'india'),\n",
      "   (0.0045570345, 'member')],\n",
      "  -3.166759000594137),\n",
      " ([(0.03232836, 'game'),\n",
      "   (0.022659902, 'team'),\n",
      "   (0.016557481, 'sport'),\n",
      "   (0.013400229, 'player'),\n",
      "   (0.011744707, 'football'),\n",
      "   (0.011733152, 'world'),\n",
      "   (0.011466979, 'first'),\n",
      "   (0.010546175, 'gland'),\n",
      "   (0.010185586, 'cup'),\n",
      "   (0.00994783, 'basketball'),\n",
      "   (0.008932636, 'time'),\n",
      "   (0.0085159, 'play'),\n",
      "   (0.008288459, 'seed'),\n",
      "   (0.008041462, 'ball'),\n",
      "   (0.007679483, 'season'),\n",
      "   (0.0072800224, 'win'),\n",
      "   (0.007124263, 'four'),\n",
      "   (0.0070557273, 'record'),\n",
      "   (0.006830879, 'two'),\n",
      "   (0.0067309584, 'club')],\n",
      "  -3.4580310996397703),\n",
      " ([(0.017768694, 'song'),\n",
      "   (0.016917879, 'music'),\n",
      "   (0.016843563, 'single'),\n",
      "   (0.01663084, 'carbon'),\n",
      "   (0.01233063, 'number'),\n",
      "   (0.012000482, 'nucleus'),\n",
      "   (0.011040878, 'one'),\n",
      "   (0.010265468, 'hydrogen'),\n",
      "   (0.008857497, 'released'),\n",
      "   (0.008327459, 'rock'),\n",
      "   (0.0073811216, 'fun'),\n",
      "   (0.007048284, 'hit'),\n",
      "   (0.006894053, 'healthy'),\n",
      "   (0.0068729827, 'written'),\n",
      "   (0.0062019634, 'heard'),\n",
      "   (0.0060665957, 'tried'),\n",
      "   (0.0060542338, 'hot'),\n",
      "   (0.0060171355, 'band'),\n",
      "   (0.005533678, 'album'),\n",
      "   (0.0055031143, 'first')],\n",
      "  -4.2917787092287405),\n",
      " ([(0.020489493, 'film'),\n",
      "   (0.015298578, 'series'),\n",
      "   (0.012657697, 'star'),\n",
      "   (0.012656382, 'show'),\n",
      "   (0.011008038, 'american'),\n",
      "   (0.0100217415, 'season'),\n",
      "   (0.009422744, 'born'),\n",
      "   (0.009399576, 'role'),\n",
      "   (0.009111959, 'movie'),\n",
      "   (0.00881795, 'television'),\n",
      "   (0.008399927, 'please'),\n",
      "   (0.007873667, 'best'),\n",
      "   (0.007824118, 'character'),\n",
      "   (0.006673808, 'dad'),\n",
      "   (0.0057004304, 'tv'),\n",
      "   (0.0056589623, 'known'),\n",
      "   (0.0052910466, 'also'),\n",
      "   (0.0052556777, 'episode'),\n",
      "   (0.0051413593, 'amino_acid'),\n",
      "   (0.00510177, 'watching')],\n",
      "  -4.309975679121137),\n",
      " ([(0.014570056, 'water'),\n",
      "   (0.0136290295, 'atom'),\n",
      "   (0.011628393, 'earth'),\n",
      "   (0.011515795, 'reaction'),\n",
      "   (0.011493318, 'electron'),\n",
      "   (0.010823128, 'area'),\n",
      "   (0.00943772, 'wave'),\n",
      "   (0.007950262, 'chemical'),\n",
      "   (0.0070381816, 'fish'),\n",
      "   (0.0066157295, 'park'),\n",
      "   (0.0064901314, 'sun'),\n",
      "   (0.0060402756, 'ocean'),\n",
      "   (0.0058914903, 'gene'),\n",
      "   (0.005376514, 'sea'),\n",
      "   (0.005363415, 'light'),\n",
      "   (0.0052885963, 'chromosome'),\n",
      "   (0.005205031, 'dna'),\n",
      "   (0.005113818, 'land'),\n",
      "   (0.0048931544, 'tree'),\n",
      "   (0.0048569716, 'river')],\n",
      "  -5.298154418547653),\n",
      " ([(0.019037405, 'chinese'),\n",
      "   (0.018927064, 'china'),\n",
      "   (0.015986722, 'people'),\n",
      "   (0.01564184, 'world'),\n",
      "   (0.015631825, 'country'),\n",
      "   (0.015154269, 'english'),\n",
      "   (0.013804646, 'compound'),\n",
      "   (0.0137512265, 'membrane'),\n",
      "   (0.013186879, 'substance'),\n",
      "   (0.012643979, 'ion'),\n",
      "   (0.011987711, 'language'),\n",
      "   (0.009704497, 'hormone'),\n",
      "   (0.009501202, 'population'),\n",
      "   (0.0093046, 'metal'),\n",
      "   (0.008844336, 'grow'),\n",
      "   (0.007502593, 'million'),\n",
      "   (0.006901014, 'many_people'),\n",
      "   (0.006428692, 'beijing'),\n",
      "   (0.005859864, 'volume'),\n",
      "   (0.0056915698, 'nuclear')],\n",
      "  -5.850854491636995)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert another corpus to the LDA space and index it.\n",
    "index = similarities.MatrixSimilarity(model[corpus])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_prediction(index, corpus):\n",
    "    preds = []\n",
    "    data_idx = [89453, 89453+18613, 89453+18613+11679]\n",
    "    for d in corpus:\n",
    "        p_idx = np.argmax(index[model[d]])\n",
    "        for idx, domain_idx in enumerate(data_idx):\n",
    "            if p_idx < domain_idx:\n",
    "                preds.append(idx)\n",
    "                break\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_pred = domain_prediction(index, nq_test_corpus)\n",
    "race_pred = domain_prediction(index, race_test_corpus)\n",
    "sciq_pred = domain_prediction(index, sciq_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83474155 0.79383634 0.69134253] [0.90150295 0.72104247 0.551     ] [0.86683871 0.75569044 0.6132443 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "ground_truth = [0]*len(nq_pred) + [1]*len(race_pred) + [2] * len(sciq_pred)\n",
    "preds = nq_pred + race_pred + sciq_pred\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ground_truth, preds, average=None)\n",
    "print(precision, recall, f1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domain_selection(index, corpus, num_keeped=1):\n",
    "    num_nq_ds = 89453\n",
    "    selection = []\n",
    "    for d in corpus:\n",
    "        nq_scores = index[model[d]][:num_nq_ds]\n",
    "        p_idx = np.argsort(nq_scores)[::-1]\n",
    "        selection.extend([nq_ds[_] for _ in p_idx[:num_keeped]])\n",
    "    return selection\n",
    "\n",
    "d = np.arange(len(race_corpus))\n",
    "np.random.shuffle(d)\n",
    "f = open(\"../../data/nq/race_lda/1000.jsonl\", \"w\")\n",
    "race_selected = domain_selection(index, [race_corpus[_] for _ in d[:1000]]) # race_test_corpus)\n",
    "for w in race_selected:\n",
    "    f.write(w)\n",
    "f.close()\n",
    "\n",
    "d = np.arange(len(sciq_corpus))\n",
    "np.random.shuffle(d)\n",
    "f = open(\"../../data/nq/sciq_lda/1000.jsonl\", \"w\")\n",
    "sciq_selected = domain_selection(index, [sciq_corpus[_] for _ in d[:1000]]) # sciq_test_corpus)\n",
    "for w in sciq_selected:\n",
    "    f.write(w)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e232a1ed4a4615ad5770d495df72e13eb542862ed083a40bb2c63f4b52e84f8a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('cogqg': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
